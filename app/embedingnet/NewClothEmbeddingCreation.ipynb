{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import Resnet_18\n",
    "from tripletnet import Tripletnet\n",
    "from type_specific_network import TypeSpecificNet\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.12_2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--sim_i_loss'], dest='sim_i_loss', nargs=None, const=None, default=5e-05, type=<type 'float'>, choices=None, help='parameter for loss for image-image similarity', metavar='M')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Fashion Compatibility Example')\n",
    "parser.add_argument('--batch-size', type=int, default=256, metavar='N',\n",
    "                    help='input batch size for training (default: 256)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--start_epoch', type=int, default=1, metavar='N',\n",
    "                    help='number of start epoch (default: 1)')\n",
    "parser.add_argument('--lr', type=float, default=5e-5, metavar='LR',\n",
    "                    help='learning rate (default: 5e-5)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--log-interval', type=int, default=250, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--resume', default='', type=str,\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--name', default='Type_Specific_Fashion_Compatibility', type=str,\n",
    "                    help='name of experiment')\n",
    "parser.add_argument('--polyvore_split', default='nondisjoint', type=str,\n",
    "                    help='specifies the split of the polyvore data (either disjoint or nondisjoint)')\n",
    "parser.add_argument('--datadir', default='data', type=str,\n",
    "                    help='directory of the polyvore outfits dataset (default: data)')\n",
    "parser.add_argument('--test', dest='test', action='store_true', default=False,\n",
    "                    help='To only run inference on test set')\n",
    "parser.add_argument('--dim_embed', type=int, default=64, metavar='N',\n",
    "                    help='how many dimensions in embedding (default: 64)')\n",
    "parser.add_argument('--use_fc', action='store_true', default=False,\n",
    "                    help='Use a fully connected layer to learn type specific embeddings.')\n",
    "parser.add_argument('--learned', dest='learned', action='store_true', default=False,\n",
    "                    help='To learn masks from random initialization')\n",
    "parser.add_argument('--prein', dest='prein', action='store_true', default=False,\n",
    "                    help='To initialize masks to be disjoint')\n",
    "parser.add_argument('--rand_typespaces', action='store_true', default=False,\n",
    "                    help='randomly assigns comparisons to type-specific embeddings where #comparisons < #embeddings')\n",
    "parser.add_argument('--num_rand_embed', type=int, default=4, metavar='N',\n",
    "                    help='number of random embeddings when rand_typespaces=True')\n",
    "parser.add_argument('--l2_embed', dest='l2_embed', action='store_true', default=False,\n",
    "                    help='L2 normalize the output of the type specific embeddings')\n",
    "parser.add_argument('--learned_metric', dest='learned_metric', action='store_true', default=False,\n",
    "                    help='Learn a distance metric rather than euclidean distance')\n",
    "parser.add_argument('--margin', type=float, default=0.3, metavar='M',\n",
    "                    help='margin for triplet loss (default: 0.2)')\n",
    "parser.add_argument('--embed_loss', type=float, default=5e-4, metavar='M',\n",
    "                    help='parameter for loss for embedding norm')\n",
    "parser.add_argument('--mask_loss', type=float, default=5e-4, metavar='M',\n",
    "                    help='parameter for loss for mask norm')\n",
    "parser.add_argument('--vse_loss', type=float, default=5e-3, metavar='M',\n",
    "                    help='parameter for loss for the visual-semantic embedding')\n",
    "parser.add_argument('--sim_t_loss', type=float, default=5e-5, metavar='M',\n",
    "                    help='parameter for loss for text-text similarity')\n",
    "parser.add_argument('--sim_i_loss', type=float, default=5e-5, metavar='M',\n",
    "                    help='parameter for loss for image-image similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x103871ab0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global args\n",
    "args = parser.parse_args(args=['--test', \"--l2_embed\", \"--resume\", \"runs/nondisjoint_l2norm/model_best.pth.tar\"])\n",
    "args.cuda = False #not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.datadir=datadir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs/nondisjoint_l2norm/model_best.pth.tar'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"typespaces.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = df['0'].values\n",
    "values = df['1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "typespaces=dict(zip(keys,values))\n",
    "text_feature_dim = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Resnet_18.resnet18(pretrained=True, embedding_size=args.dim_embed)\n",
    "csn_model = TypeSpecificNet(args, model, len(typespaces))\n",
    "\n",
    "criterion = torch.nn.MarginRankingLoss(margin = args.margin)\n",
    "tnet = Tripletnet(args, csn_model, text_feature_dim, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'runs/nondisjoint_l2norm/model_best.pth.tar'\n",
      "=> loaded checkpoint 'runs/nondisjoint_l2norm/model_best.pth.tar' (epoch 5)\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(args.resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "        checkpoint = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_acc = checkpoint['best_prec1']\n",
    "        tnet.load_state_dict(checkpoint['state_dict'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                .format(args.resume, checkpoint['epoch']))\n",
    "else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(args.resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_codes = [\n",
    " 'null',\n",
    " 'bag',\n",
    " 'belt',\n",
    " 'boots',\n",
    " 'footwear',\n",
    " 'outer',\n",
    " 'dress',\n",
    " 'sunglasses',\n",
    " 'pants',\n",
    " 'top',\n",
    " 'shorts',\n",
    " 'skirt',\n",
    " 'headwear',\n",
    " 'scarf & tie']\n",
    "\n",
    "new_codes = ['tops',\n",
    " 'shoes',\n",
    " 'all-body',\n",
    " 'scarves',\n",
    " 'outerwear',\n",
    " 'accessories',\n",
    " 'hats',\n",
    " 'bags',\n",
    " 'bottoms',\n",
    " 'sunglasses']\n",
    "\n",
    "matches = {\n",
    " 'bag':'bags',\n",
    " 'belt':'accessories',\n",
    " 'boots':'shoes',\n",
    " 'footwear':'shoes',\n",
    " 'outer':'outerwear',\n",
    " 'dress':'all-body',\n",
    " 'sunglasses':'sunglasses',\n",
    " 'pants':'bottoms',\n",
    " 'top':'tops',\n",
    " 'shorts':'bottoms',\n",
    " 'skirt':'bottoms',\n",
    " 'headwear':'hats',\n",
    " 'scarf & tie':'scarves'}\n",
    "\n",
    "old_code_to_new_and_name = {i + 1 : (new_codes.index(matches[name]) + 1, matches[name]) for i, name in enumerate(old_codes[1:])}\n",
    "\n",
    "def get_parts_by_mask(img, mask_true):\n",
    "    mask = np.copy(mask_true)\n",
    "    mask_ = np.copy(mask)\n",
    "    for key, value in old_code_to_new_and_name.items():\n",
    "        #if key == 7:\n",
    "        #   print(value)\n",
    "        mask[mask_==key] = value[0]\n",
    "    classes = set(mask.flatten())\n",
    "    #print(classes)\n",
    "    resized_mask = np.round(np.array(Image.fromarray(np.uint8(mask), 'L').resize(reversed(img.shape[:2]))))\n",
    "    classes = list(set(mask.flatten()))\n",
    "    if 0 in classes:\n",
    "        classes.remove(0)\n",
    "    #print('classes:', classes)\n",
    "    final_imgs = []\n",
    "    for cls in classes:\n",
    "        #print(cls, new_codes[cls - 1])\n",
    "        curr_mask = (resized_mask == cls) * 1.\n",
    "        new_img = np.zeros(img.shape)\n",
    "        new_img[:, :, 0] = np.multiply(img[:, :, 0], curr_mask)\n",
    "        new_img[:, :, 1] = np.multiply(img[:, :, 1], curr_mask)\n",
    "        new_img[:, :, 2] = np.multiply(img[:, :, 2], curr_mask)\n",
    "        new_img[new_img==0] = 255.\n",
    "        new_img = new_img.astype(np.uint8)\n",
    "        axis0 = np.where(curr_mask.sum(axis=0) > 0)[0]\n",
    "        axis1 = np.where(curr_mask.sum(axis=1) > 0)[0]\n",
    "        min_x, max_x = min(axis0), max(axis0)\n",
    "        min_y, max_y = min(axis1), max(axis1)\n",
    "        new_img = new_img[min_y : max_y, min_x : max_x]\n",
    "        N, M, _ = new_img.shape\n",
    "        d = int(max(N, M) / 10)\n",
    "        D = max(N, M) + 2 * d\n",
    "        img_to_reshape = np.ones((D, D, 3)) * 255.\n",
    "        if N > M:\n",
    "            y = d + int((D - M) / 2)\n",
    "            img_to_reshape[d:N + d, y - d:y - d + M, :] = new_img\n",
    "        else:\n",
    "            x = d + int((D - N) / 2)\n",
    "            img_to_reshape[x - d : x - d + N, d : M + d] = new_img\n",
    "        img_to_reshape = img_to_reshape.astype(np.uint8)\n",
    "        sized_img = Image.fromarray(img_to_reshape).resize((112, 112))\n",
    "        final_imgs.append((sized_img, new_codes[cls - 1]))\n",
    "    return final_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (8, 'bags'),\n",
       " 2: (6, 'accessories'),\n",
       " 3: (2, 'shoes'),\n",
       " 4: (2, 'shoes'),\n",
       " 5: (5, 'outerwear'),\n",
       " 6: (3, 'all-body'),\n",
       " 7: (10, 'sunglasses'),\n",
       " 8: (9, 'bottoms'),\n",
       " 9: (1, 'tops'),\n",
       " 10: (9, 'bottoms'),\n",
       " 11: (9, 'bottoms'),\n",
       " 12: (7, 'hats'),\n",
       " 13: (4, 'scarves')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_code_to_new_and_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"/Users/nikitakaraev/Desktop/PROJET_DEEP_LEARNING/FACEBOOKHACK/0000731.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=Image.open(\"/Users/nikitakaraev/Desktop/PROJET_DEEP_LEARNING/FACEBOOKHACK/0000731.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "imarr=np.array(im)\n",
    "maskarr=np.array(mask).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 400, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imarr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10d0c5dd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMMAAAD8CAYAAADKUxDSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b45fd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(maskarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 400)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskarr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloth_images = get_parts_by_mask(imarr, maskarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<PIL.Image.Image image mode=RGB size=112x112 at 0x10D0CD650>, 'tops'),\n",
       " (<PIL.Image.Image image mode=RGB size=112x112 at 0x10B46D810>, 'shoes'),\n",
       " (<PIL.Image.Image image mode=RGB size=112x112 at 0x10B46D710>, 'accessories'),\n",
       " (<PIL.Image.Image image mode=RGB size=112x112 at 0x10B46D9D0>, 'bottoms'),\n",
       " (<PIL.Image.Image image mode=RGB size=112x112 at 0x10B46DA10>, 'sunglasses')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloth_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAIAAABJgmMcAAAHqUlEQVR4nO2bTYhkVxXHf/fe96q6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\nCvMvhG7EI+NMVaAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=112x112 at 0x10B46DA10>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloth_images[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform=transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedImage = transform(cloth_images[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "  2.2489  2.2489  2.2489  ...   2.2489  2.2489  2.2489\n",
       "  2.2489  2.2489  2.2489  ...   2.2489  2.2489  2.2489\n",
       "  2.2489  2.2489  2.2489  ...   2.2489  2.2489  2.2489\n",
       "           ...             ⋱             ...          \n",
       "  2.2489  2.2489  2.2489  ...   2.2489  2.2489  2.2489\n",
       "  2.2489  2.2489  2.2489  ...   2.2489  2.2489  2.2489\n",
       "  2.2489  2.2489  2.2489  ...   2.2489  2.2489  2.2489\n",
       "\n",
       "( 1 ,.,.) = \n",
       "  2.4286  2.4286  2.4286  ...   2.4286  2.4286  2.4286\n",
       "  2.4286  2.4286  2.4286  ...   2.4286  2.4286  2.4286\n",
       "  2.4286  2.4286  2.4286  ...   2.4286  2.4286  2.4286\n",
       "           ...             ⋱             ...          \n",
       "  2.4286  2.4286  2.4286  ...   2.4286  2.4286  2.4286\n",
       "  2.4286  2.4286  2.4286  ...   2.4286  2.4286  2.4286\n",
       "  2.4286  2.4286  2.4286  ...   2.4286  2.4286  2.4286\n",
       "\n",
       "( 2 ,.,.) = \n",
       "  2.6400  2.6400  2.6400  ...   2.6400  2.6400  2.6400\n",
       "  2.6400  2.6400  2.6400  ...   2.6400  2.6400  2.6400\n",
       "  2.6400  2.6400  2.6400  ...   2.6400  2.6400  2.6400\n",
       "           ...             ⋱             ...          \n",
       "  2.6400  2.6400  2.6400  ...   2.6400  2.6400  2.6400\n",
       "  2.6400  2.6400  2.6400  ...   2.6400  2.6400  2.6400\n",
       "  2.6400  2.6400  2.6400  ...   2.6400  2.6400  2.6400\n",
       "[torch.FloatTensor of size 3x112x112]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ims=Variable(transformedImage); ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tnet.embeddingnet(ims.unsqueeze(0)).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 67, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-test",
   "language": "python",
   "name": "py36-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
